---
title: "Problem Set 6"
format: html
editor: visual
embed-resources: true
execute:
  error: false
---

GitHub link: <https://github.com/IceCedar/STATS506>

## Stratified Bootstrapping

### a.

```{r}
library(DBI)
library(RSQLite)
library(dplyr)
lahman<-dbConnect(RSQLite::SQLite(),"lahman_1871-2022.sqlite")

fielding<-dbGetQuery(lahman,"SELECT * FROM fielding")

#calculate the average RF for each team
field_RF<-fielding %>% filter(InnOuts != 0) %>%
  mutate(RF=3*(PO+A)/(InnOuts)) 
field_team<-field_RF%>%
  group_by(teamID) %>%
  summarize(average_RF=mean(RF,na.rm=TRUE)) %>%
  ungroup()

field_team
```

### 1. Without any Parallel processing

```{r}
team<-unique(field_RF$teamID)

# Define the function
boot1 <- function(n) {
  estimate_team <- numeric(length(team))
  
  #Use lapply to calculate bootstrap standard deviations for each team
  boot_results <- lapply(team, function(t) {
    dat <- field_RF$RF[field_RF$teamID == t]
    
    boot_sds <- numeric(n)
    
    # Perform bootstrap sampling and calculate standard deviations
    for (i in 1:n) {
      temp <- sample(dat, length(dat), replace = TRUE)
      boot_sds[i] <- sd(temp, na.rm = TRUE)
    }
    mean(boot_sds, na.rm = TRUE)
  })
  
  estimate_team <- boot_results
  
  # Add the estimated standard deviations to field_team
  field_team$sd_RF <- unlist(estimate_team)
  
  return(field_team)
}

```

### 2. Using parallel processing

```{r}
library(parallel)

boot2<-function(n){
  
  cl <- makeCluster(8)
  clusterExport(cl, varlist = c("field_RF", "field_team", "team", "n"))
  
  # Use parLapply to calculate bootstrap standard deviations for each team
  boot_results <- parLapply(cl, team, function(t) {
    dat <- field_RF$RF[field_RF$teamID == t]
    boot_sds <- numeric(n)

    # Perform bootstrap sampling and calculate standard deviations
    for (i in 1:n) {
      temp <- sample(dat, length(dat), replace = TRUE)
      boot_sds[i] <- sd(temp, na.rm = TRUE)
    }
    
    # Return the mean of the bootstrap standard deviations
    mean(boot_sds, na.rm = TRUE)
  })
  
  stopCluster(cl)
  field_team$sd_RF <- unlist(boot_results)
  
  return(field_team)
}

```

### 3. Using futures with the future package

```{r}
library(future)
library(future.apply)
library(data.table)

boot3 <- function(n) {
  # Set up parallel computation plan and number of workers
  plan(multisession, workers = 8) 

  # Use future_lapply for parallel computation
  results <- future_lapply(seq_along(team), function(i) {
    t <- team[i]
    dat <- data.table(field_RF$RF[field_RF$teamID == t])
    
    # Efficient matrix-based bootstrap sampling
    boot_samples <- matrix(sample(dat$V1, n * nrow(dat), replace = TRUE), ncol = n)
    boot_sds <- apply(boot_samples, 2, sd, na.rm = TRUE)
    
    mean(boot_sds, na.rm = TRUE)
  }, future.seed = TRUE)

  # Combine the results into the field_team data frame
  field_team <- field_team %>% mutate(sd_RF = unlist(results))
  return(field_team)
}

```

### b.

### 1. Without any Parallel processing

```{r}
field_team1<-boot1(1000)
field_team1 %>%
  arrange(desc(average_RF)) %>%  # Order by average_RF
  head(10) 
```

### 2. Using parallel processing

```{r}
field_team2<-boot2(1000)
field_team2 %>%
  arrange(desc(average_RF)) %>%  # Order by average_RF
  head(10)
```

### 3. Using futures with the future package

```{r}
field_team3<-boot3(1000)
field_team3 %>%
  arrange(desc(average_RF)) %>%  # Order by average_RF
  head(10)
```

### c.

```{r}
basic<-system.time({
  boot1(1000)  # Without any parallel processing
})

Parrallel<-system.time({
  boot2(1000)  # Using parallel processing
})

Future<-system.time({
  boot3(1000)  # Using futures with the future package
})

rbind(basic,Parrallel,Future)
```
```{r}
#Compare different sample size
basic<-system.time({
  boot1(100)  # Without any parallel processing
})

Parrallel<-system.time({
  boot2(100)  # Using parallel processing
})

Future<-system.time({
  boot3(100)  # Using futures with the future package
})

rbind(basic,Parrallel,Future)
```

The basic method demonstrates good performance with small sample sizes; however, it becomes inefficient for larger datasets due to its single-threaded nature. While employing parallel processing and futures, the efficiency gains are minimal for smaller sample size. In contrast, when handling larger sample sizes, the advantages of both parallelization and futures become pronounced. These methods significantly reduce computation time and improve resource utilization compared to the basic method, making them more suitable for large-scale data analysis. Overall, the choice of method should consider the sample size, as parallel processing and futures are more effective in scenarios involving larger datasets.
